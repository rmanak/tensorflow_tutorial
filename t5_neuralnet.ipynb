{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonlinear_data(size=3000, kind='circles', factor=0.3, noise=0.2):\n",
    "    if kind == 'circles':\n",
    "        return sklearn.datasets.make_circles(n_samples=size, factor=factor, noise=noise)\n",
    "    \n",
    "    if kind == 'moons':\n",
    "        return sklearn.datasets.make_moons(n_samples=size, noise=noise)\n",
    "    \n",
    "    raise ValueError(f\"unknown kind {kind}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = get_nonlinear_data(kind='moons', noise=0.2)\n",
    "plt.scatter(X[:,0], X[:, 1], c=labels, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(activation='relu', hidden_units=[10, 10], \n",
    "              input_shape=(None, 2), output_shape=(None, 2)):\n",
    "    \n",
    "    activation_map = {'sigmoid': tf.sigmoid, 'relu': tf.nn.relu}\n",
    "    \n",
    "    input_X = tf.placeholder(shape=input_shape, name='input_X', dtype=tf.float32)\n",
    "    output_Y = tf.placeholder(shape=output_shape, name='output_X', dtype=tf.float32)\n",
    "    curr_layer = input_X\n",
    "    curr_size = input_shape[1]\n",
    "    for l, size in enumerate(hidden_units):\n",
    "        hidden_layer_W = tf.get_variable(name=f'HiddenLayerW{l}', \n",
    "                                         shape=(curr_size, size), \n",
    "                                         initializer=tf.random_normal_initializer(seed=0))\n",
    "        \n",
    "        hidden_layer_b = tf.get_variable(name=f'HiddenLayerB{l}', \n",
    "                                         shape=(1, size), \n",
    "                                         initializer=tf.random_normal_initializer(seed=0))\n",
    "        \n",
    "        curr_layer = activation_map[activation](tf.matmul(curr_layer, hidden_layer_W) + hidden_layer_b)\n",
    "        curr_size = size\n",
    "    \n",
    "    softmax_layer_W = tf.get_variable(name='SoftmaxLayerW', shape=(curr_size, output_shape[1]),\n",
    "                                     initializer=tf.random_normal_initializer(seed=0))\n",
    "    \n",
    "    softmax_layer_b = tf.get_variable(name='SoftmaxLayerB', shape=(1, output_shape[1]),\n",
    "                                     initializer=tf.random_normal_initializer(seed=0))\n",
    "    \n",
    "    curr_layer = tf.nn.softmax(tf.matmul(curr_layer, softmax_layer_W) + softmax_layer_b)\n",
    "    \n",
    "    return input_X, output_Y, curr_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X, output_Y, curr_layer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(tf.reduce_mean(output_Y*tf.log(curr_layer), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, y, size=100):\n",
    "    counter = 0\n",
    "    while counter < x.shape[0]:\n",
    "        yield x[counter:counter+size, :], y[counter:counter+size, :]\n",
    "        counter += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = OneHotEncoder().fit_transform(labels.reshape(len(labels), 1)).toarray()\n",
    "\n",
    "EPOCH = 1000\n",
    "BATHC_SIZE = 100\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(EPOCH):\n",
    "        for x_batch, y_batch in batchify(X, Y):\n",
    "            loss_val, _ = sess.run([loss, optimizer], feed_dict={input_X: x_batch, output_Y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch {} batch loss {}\".format(epoch, loss_val), end='')\n",
    "            loss_val, class_probs = sess.run([loss, curr_layer], feed_dict={input_X: X, output_Y: Y})\n",
    "            pred_label = np.argmax(class_probs, axis=1)\n",
    "            acc = accuracy_score(labels, pred_label)\n",
    "            print(\" total loss {} accuray {}\".format(loss_val, acc))\n",
    "    print(\"last batch loss {}\".format(loss_val), end='')\n",
    "    loss_val, class_probs = sess.run([loss, curr_layer], feed_dict={input_X: X, output_Y: Y})\n",
    "    pred_label = np.argmax(class_probs, axis=1)\n",
    "    acc = accuracy_score(labels, pred_label)\n",
    "    print(\" total loss {} accuray {}\".format(loss_val, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.squeeze(class_probs[:, 1])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=probs, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
