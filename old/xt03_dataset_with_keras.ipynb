{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import sys\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from string import ascii_lowercase\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import (Input, \n",
    "                                     Dense, \n",
    "                                     Concatenate,\n",
    "                                     AlphaDropout,\n",
    "                                     Conv1D,\n",
    "                                     GlobalMaxPooling1D,\n",
    "                                     MaxPooling1D)\n",
    "                                     \n",
    "\n",
    "TEMP_DIR = '/tmp/tensorflow_tutorials'\n",
    "WORD_CHARS = set(ascii_lowercase + \"'!?-.()\")\n",
    "\n",
    "def download_and_cache(url, fname=None, dest=TEMP_DIR):\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "    if fname is None:\n",
    "        fname = url.split('/')[-1]\n",
    "    fpath = os.path.join(dest, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            percentage = float(count * block_size) / float(total_size) * 100.0\n",
    "            sys.stdout.write('\\r>> Downloading {} {:1.1f}%'.format(fname, percentage))\n",
    "            sys.stdout.flush()\n",
    "        fpath, _ = urllib.request.urlretrieve(url, fpath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(fpath)\n",
    "        print('Successfully downloaded', fname, statinfo.st_size, 'bytes.')\n",
    "    return fpath\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = download_and_cache('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(fpath, 'r:gz') as tar:\n",
    "    tar.extractall(TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = glob.glob(os.path.join(TEMP_DIR, 'aclImdb', 'train/pos/', '*.txt'))\n",
    "train_neg = glob.glob(os.path.join(TEMP_DIR, 'aclImdb', 'train/neg/', '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = train_pos + train_neg\n",
    "labels = [1]*len(train_pos) + [0]*len(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "MAX_INPUT_LEN = 1024\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "ALPHABET_SIZE = len(ALPHABET)\n",
    "\n",
    "CHAR_ID = dict()\n",
    "for idx, char_ in enumerate(ALPHABET):\n",
    "    CHAR_ID[char_] = idx + 1\n",
    "    \n",
    "def str_to_array(s, input_size=MAX_INPUT_LEN):\n",
    "    \"\"\"\n",
    "    Converting string characters to integer index according to CHAR_ID\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    str_index = np.zeros(input_size, dtype='int64')\n",
    "    max_len = min(len(s), input_size)\n",
    "    for i in range(1, max_len + 1):\n",
    "        str_index[i-1] = CHAR_ID.get(s[-i], 0)\n",
    "    return str_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_word(word):\n",
    "    return set(word.lower()).issubset(WORD_CHARS)\n",
    "\n",
    "def _preprocess_text(input_text, label):\n",
    "    soup = BeautifulSoup(input_text, \"lxml\")\n",
    "    sents = nltk.sent_tokenize(soup.get_text())\n",
    "    words = [nltk.word_tokenize(sent) for sent in sents]\n",
    "    res = ' '.join(' '.join(word.lower() for word in sent_word if _is_word(word)) for sent_word in words)\n",
    "    return str_to_array(res), label\n",
    "    \n",
    "def _read_files(filename, label):\n",
    "    file_content = tf.read_file(filename)\n",
    "    return file_content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(_read_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_func(text, label):\n",
    "    return tuple(tf.py_function(_preprocess_text, [text, label], [tf.string, label.dtype]))\n",
    "dataset = dataset.map(wrapped_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=1024).batch(128).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(conv_layers,\n",
    "                      fully_connected_layers,\n",
    "                      input_size=MAX_INPUT_LEN,\n",
    "                      embedding_size=32,\n",
    "                      alphabet_size=ALPHABET_SIZE,\n",
    "                      num_classes=2, optimizer='adam',\n",
    "                      dropout_proba=0.5, fl_activation='selu',\n",
    "                      fl_initializer='lecun_normal',\n",
    "                      conv_activations='tanh',\n",
    "                      loss='categorical_crossentropy'):\n",
    "    \"\"\"\n",
    "    Based on: https://arxiv.org/abs/1508.06615\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_size,), name='input_layer', dtype='int64')\n",
    "    embeds = Embedding(alphabet_size + 1, embedding_size, input_length=input_size)(inputs)\n",
    "    convs = list()\n",
    "    for num_filters, filter_width in conv_layers:\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                             kernel_size=filter_width,\n",
    "                             activation=conv_activations,\n",
    "                             name='ConvLayer{}{}'.format(num_filters, filter_width))(embeds)\n",
    "        pool = GlobalMaxPooling1D(name='MaxPoolLayer{}{}'.format(num_filters, filter_width))(conv)\n",
    "        convs.append(pool)\n",
    "\n",
    "    x = Concatenate()(convs)\n",
    "    for units in fully_connected_layers:\n",
    "        x = Dense(units, activation=fl_activation, kernel_initializer=fl_initializer)(x)\n",
    "        x = AlphaDropout(dropout_proba)(x)\n",
    "\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model([(16, 9), (16, 7), (16, 5), (16, 3)], [32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This doesn't work!!\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
