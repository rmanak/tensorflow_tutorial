{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import sys\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from string import ascii_lowercase\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "TEMP_DIR = '/tmp/tensorflow_tutorials'\n",
    "WORD_CHARS = set(ascii_lowercase + \"'!?-.()\")\n",
    "\n",
    "def download_and_cache(url, fname=None, dest=TEMP_DIR):\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "    if fname is None:\n",
    "        fname = url.split('/')[-1]\n",
    "    fpath = os.path.join(dest, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            percentage = float(count * block_size) / float(total_size) * 100.0\n",
    "            sys.stdout.write('\\r>> Downloading {} {:1.1f}%'.format(fname, percentage))\n",
    "            sys.stdout.flush()\n",
    "        fpath, _ = urllib.request.urlretrieve(url, fpath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(fpath)\n",
    "        print('Successfully downloaded', fname, statinfo.st_size, 'bytes.')\n",
    "    return fpath\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpath = download_and_cache('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tarfile.open(fpath, 'r:gz') as tar:\n",
    "    tar.extractall(TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos = glob.glob(os.path.join(TEMP_DIR, 'aclImdb', 'train/pos/', '*.txt'))\n",
    "train_neg = glob.glob(os.path.join(TEMP_DIR, 'aclImdb', 'train/neg/', '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = train_pos + train_neg\n",
    "labels = [1]*len(train_pos) + [0]*len(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _is_word(word):\n",
    "    return set(word.lower()).issubset(WORD_CHARS)\n",
    "\n",
    "def _preprocess_text(input_text, label):\n",
    "    soup = BeautifulSoup(input_text, \"lxml\")\n",
    "    sents = nltk.sent_tokenize(soup.get_text())\n",
    "    words = [nltk.word_tokenize(sent) for sent in sents]\n",
    "    res = ' '.join(' '.join(word.lower() for word in sent_word if _is_word(word)) for sent_word in words)\n",
    "    return res, label\n",
    "    \n",
    "def _read_files(filename, label):\n",
    "    file_content = tf.read_file(filename)\n",
    "    return file_content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(_read_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrapped_func(text, label):\n",
    "    return tuple(tf.py_func(_preprocess_text, [text, label], [tf.string, label.dtype]))\n",
    "dataset = dataset.map(wrapped_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    res = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(filenames[0], 'r') as fp:\n",
    "    print(fp.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other resources:\n",
    "# https://cs230-stanford.github.io/tensorflow-input-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
