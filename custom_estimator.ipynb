{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEMP_DIR = '/tmp/tensorflow_tutorials'\n",
    "\n",
    "def download_and_cache(url, fname=None, dest=TEMP_DIR):\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "    if fname is None:\n",
    "        fname = url.split('/')[-1]\n",
    "        print(\"Using fname:\", fname)\n",
    "    fpath = os.path.join(dest, fname)\n",
    "    if not os.path.exists(fpath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            percentage = float(count * block_size) / float(total_size) * 100.0\n",
    "            sys.stdout.write('\\r>> Downloading {} {:1.1f}%'.format(fname, percentage))\n",
    "            sys.stdout.flush()\n",
    "        fpath, _ = urllib.request.urlretrieve(url, fpath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(fpath)\n",
    "        print('Successfully downloaded', fname, statinfo.st_size, 'bytes.')\n",
    "    return fpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "\n",
    "FEAT_COLUMNS = ['SepalLength', 'SepalWidth',\n",
    "                'PetalLength', 'PetalWidth']\n",
    "\n",
    "def _parse_line(line):\n",
    "    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\n",
    "    features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "    label = features.pop('Species')\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def csv_input_fn(csv_path, batch_size, shuffle_repeat=True):\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(csv_path).skip(1)\n",
    "    dataset = dataset.map(_parse_line)\n",
    "    \n",
    "    # no need to repeat and shuffle during eval or pred mode\n",
    "    if shuffle_repeat:\n",
    "        dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def input_fn(url, shuffle_repeat=True, batch_size=100):\n",
    "    path = download_and_cache(url) \n",
    "    return csv_input_fn(path, batch_size=batch_size, \n",
    "                        shuffle_repeat=shuffle_repeat)\n",
    "\n",
    "def infer_input_fn(features, batch_size=100):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(features))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(features, feature_column, \n",
    "               hidden_units, output_units):\n",
    "    \n",
    "    layer = tf.feature_column.input_layer(features, feature_column)\n",
    "    for units in hidden_units:\n",
    "        layer = tf.layers.dense(layer, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    logits = tf.layers.dense(layer, units=output_units, activation=None)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_function(features, labels, mode, params):\n",
    "    feature_columns = params['feature_columns']\n",
    "    hidden_units = params['hidden_units']\n",
    "    output_units = params['output_layer_class_num']\n",
    "    \n",
    "    logits = neural_net(features, feature_columns, hidden_units, output_units)\n",
    "    \n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'logits': logits,\n",
    "                       'probs': 'pred_probs',\n",
    "                       'classes': pred_classes}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                    predictions=pred_classes,\n",
    "                                    name='accuracy_op')\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        metrics = {'accuracy': accuracy}\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    \"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\n",
    "    # Create three fully connected layers each layer having a dropout\n",
    "    # probability of 0.1.\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # Compute logits (1 per class).\n",
    "    logits = tf.layers.dense(net, params['output_layer_class_num'], activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    \n",
    "    metrics = {'accuracy': accuracy}\n",
    "    \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tensorflow_tutorials', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10b6b61d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(key=key) for key in FEAT_COLUMNS]\n",
    "\n",
    "model = tf.estimator.Estimator(model_function,\n",
    "                               params={\n",
    "                                   'feature_columns': feature_columns,\n",
    "                                   'hidden_units': [10, 10],\n",
    "                                   'output_layer_class_num': 3\n",
    "                               },\n",
    "                               model_dir=TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fname: iris_training.csv\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tensorflow_tutorials/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into /tmp/tensorflow_tutorials/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.063277334, step = 1001\n",
      "INFO:tensorflow:global_step/sec: 146.784\n",
      "INFO:tensorflow:loss = 0.05242203, step = 1101 (0.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.148\n",
      "INFO:tensorflow:loss = 0.08118227, step = 1201 (0.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.126\n",
      "INFO:tensorflow:loss = 0.031149482, step = 1301 (0.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.794\n",
      "INFO:tensorflow:loss = 0.032614183, step = 1401 (0.722 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.775\n",
      "INFO:tensorflow:loss = 0.031954862, step = 1501 (0.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.772\n",
      "INFO:tensorflow:loss = 0.049473893, step = 1601 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.687\n",
      "INFO:tensorflow:loss = 0.05363519, step = 1701 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.603\n",
      "INFO:tensorflow:loss = 0.01534359, step = 1801 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.952\n",
      "INFO:tensorflow:loss = 0.04652357, step = 1901 (0.646 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tensorflow_tutorials/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.050769784.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x10b6b6080>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=lambda: input_fn(TRAIN_URL), steps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can run tensorboard and see the progress:\n",
    "# !tensorboard --logdir=/tmp/tensorflow_tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fname: iris_test.csv\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-23-01:08:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tensorflow_tutorials/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-23-01:08:16\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.96666664, global_step = 2000, loss = 0.059570976\n",
      "\n",
      "Test set accuracy: 0.967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate(input_fn=lambda: input_fn(TEST_URL, shuffle_repeat=False))\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "X = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "infer_results = model.predict(input_fn=lambda: infer_input_fn(X))\n",
    "\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(iris_data.SPECIES[class_id],\n",
    "                              100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predictions[probs] must be Tensor, given: pred_probs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-17bf57d7d124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    494\u001b[0m           input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[1;32m    495\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 496\u001b[0;31m           features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    497\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m       \u001b[0mall_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cb3423243ab8>\u001b[0m in \u001b[0;36mmodel_function\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     13\u001b[0m                        \u001b[0;34m'probs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pred_probs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                        'classes': pred_classes}\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, mode, predictions, loss, train_op, eval_metric_ops, export_outputs, training_chief_hooks, training_hooks, scaffold, evaluation_hooks, prediction_hooks)\u001b[0m\n\u001b[1;32m    197\u001b[0m         predictions = {\n\u001b[1;32m    198\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_check_is_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predictions[{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         }\n\u001b[1;32m    201\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    197\u001b[0m         predictions = {\n\u001b[1;32m    198\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_check_is_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predictions[{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         }\n\u001b[1;32m    201\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m_check_is_tensor\u001b[0;34m(x, tensor_name)\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;34m\"\"\"Returns `x` if it is a `Tensor`, raises TypeError otherwise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} must be Tensor, given: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predictions[probs] must be Tensor, given: pred_probs"
     ]
    }
   ],
   "source": [
    "list(infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    \"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\n",
    "    # Create three fully connected layers each layer having a dropout\n",
    "    # probability of 0.1.\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # Compute logits (1 per class).\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    args = parser.parse_args(argv[1:])\n",
    "\n",
    "    # Fetch the data\n",
    "    (train_x, train_y), (test_x, test_y) = iris_data.load_data()\n",
    "\n",
    "    # Feature columns describe how to use the input.\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    # Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        params={\n",
    "            'feature_columns': my_feature_columns,\n",
    "            # Two hidden layers of 10 nodes each.\n",
    "            'hidden_units': [10, 10],\n",
    "            # The model must choose between 3 classes.\n",
    "            'n_classes': 3,\n",
    "        })\n",
    "\n",
    "    # Train the Model.\n",
    "    classifier.train(\n",
    "        input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\n",
    "        steps=args.train_steps)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, args.batch_size))\n",
    "\n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "    # Generate predictions from the model\n",
    "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    predict_x = {\n",
    "        'SepalLength': [5.1, 5.9, 6.9],\n",
    "        'SepalWidth': [3.3, 3.0, 3.1],\n",
    "        'PetalLength': [1.7, 4.2, 5.4],\n",
    "        'PetalWidth': [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
    "                                                labels=None,\n",
    "                                                batch_size=args.batch_size))\n",
    "\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(iris_data.SPECIES[class_id],\n",
    "                              100 * probability, expec))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
