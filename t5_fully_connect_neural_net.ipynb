{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonlinear_data(size=3000, kind='circles', faces=4, factor=0.3, noise=0.2):\n",
    "    if kind == 'circles':\n",
    "        return sklearn.datasets.make_circles(n_samples=size, factor=factor, \n",
    "                                             noise=noise)\n",
    "    \n",
    "    if kind == 'moons':\n",
    "        return sklearn.datasets.make_moons(n_samples=size, noise=noise)\n",
    "    \n",
    "    if kind == 'star':\n",
    "        X = np.zeros((size, 2))\n",
    "        Y = np.ones(size)\n",
    "        theta = np.linspace(0, 2*np.pi, size)\n",
    "        r = np.sin(theta*faces)\n",
    "        X[:, 0] = r*np.cos(theta)\n",
    "        X[:, 1] = r*np.sin(theta)\n",
    "        Y = np.mod(np.floor(theta/(2*np.pi)*faces), 2)\n",
    "        return X, Y\n",
    "    \n",
    "    if kind == 'swirly':\n",
    "        # From http://cs231n.github.io/neural-networks-case-study/\n",
    "        N = size # number of points per class\n",
    "        D = 2 # dimensionality\n",
    "        K = 2 # number of classes\n",
    "        X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "        Y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "        for j in range(K):\n",
    "            ix = range(N*j,N*(j+1))\n",
    "            r = np.linspace(0.0,1,N) # radius\n",
    "            t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "            X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "            Y[ix] = j\n",
    "        return X, Y\n",
    "    \n",
    "    raise ValueError(f\"unknown kind {kind}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = get_nonlinear_data(size=1000, kind='swirly', noise=0.2)\n",
    "plt.scatter(X[:,0], X[:, 1], c=labels, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(activation='sigmoid', hidden_units=[20], \n",
    "              input_shape=(None, 2), output_shape=(None, 2)):\n",
    "    \n",
    "    activation_map = {'sigmoid': tf.sigmoid, 'relu': tf.nn.relu}\n",
    "    \n",
    "    input_X = tf.placeholder(shape=input_shape, \n",
    "                             name='input_X', \n",
    "                             dtype=tf.float32)\n",
    "    \n",
    "    output_Y = tf.placeholder(shape=output_shape, \n",
    "                              name='output_Y', \n",
    "                              dtype=tf.float32)\n",
    "    \n",
    "    curr_layer = input_X\n",
    "    curr_size = input_shape[1]\n",
    "    for l, size in enumerate(hidden_units):\n",
    "        hidden_layer_W = tf.get_variable(name=f'HiddenLayerW{l}', \n",
    "                                         shape=(curr_size, size), \n",
    "                                         initializer=tf.random_normal_initializer(seed=0))\n",
    "        \n",
    "        hidden_layer_b = tf.get_variable(name=f'HiddenLayerB{l}', \n",
    "                                         shape=(1, size), \n",
    "                                         initializer=tf.random_normal_initializer(seed=0))\n",
    "        \n",
    "        curr_layer = activation_map[activation](tf.matmul(curr_layer, hidden_layer_W) \n",
    "                                                    + hidden_layer_b)\n",
    "        curr_size = size\n",
    "    \n",
    "    softmax_layer_W = tf.get_variable(name='SoftmaxLayerW', \n",
    "                                      shape=(curr_size, output_shape[1]),\n",
    "                                      initializer=tf.random_normal_initializer(seed=0))\n",
    "    \n",
    "    softmax_layer_b = tf.get_variable(name='SoftmaxLayerB', shape=(1, output_shape[1]),\n",
    "                                     initializer=tf.random_normal_initializer(seed=0))\n",
    "    \n",
    "    curr_layer = tf.nn.softmax(tf.matmul(curr_layer, softmax_layer_W) \n",
    "                               + softmax_layer_b)\n",
    "    \n",
    "    return input_X, output_Y, curr_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X, output_Y, curr_layer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(tf.reduce_mean(output_Y*tf.log(curr_layer), \n",
    "                       reduction_indices=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, y, size=100):\n",
    "    counter = 0\n",
    "    while counter < x.shape[0]:\n",
    "        yield x[counter:counter+size, :], y[counter:counter+size, :]\n",
    "        counter += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = OneHotEncoder().fit_transform(labels.reshape(len(labels), 1)).toarray()\n",
    "\n",
    "shuffle_index = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffle_index)\n",
    "\n",
    "X = X[shuffle_index, :]\n",
    "Y = Y[shuffle_index, :]\n",
    "\n",
    "labels = labels[shuffle_index]\n",
    "\n",
    "EPOCH = 10000\n",
    "BATCH_SIZE = 200\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(EPOCH):\n",
    "        for x_batch, y_batch in batchify(X, Y, size=BATCH_SIZE):\n",
    "            loss_val, _ = sess.run([loss, optimizer], \n",
    "                                   feed_dict={input_X: x_batch, output_Y: y_batch})\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"epoch {} batch loss {}\".format(epoch, loss_val), end='')\n",
    "            loss_val, class_probs = sess.run([loss, curr_layer], \n",
    "                                             feed_dict={input_X: X, output_Y: Y})\n",
    "            \n",
    "            pred_label = np.argmax(class_probs, axis=1)\n",
    "            acc = accuracy_score(labels, pred_label)\n",
    "            print(\" total loss {} accuray {}\".format(loss_val, acc))\n",
    "            \n",
    "    print(\"last batch loss {}\".format(loss_val), end='')\n",
    "    \n",
    "    loss_val, class_probs = sess.run([loss, curr_layer], \n",
    "                                     feed_dict={input_X: X, output_Y: Y})\n",
    "    \n",
    "    pred_label = np.argmax(class_probs, axis=1)\n",
    "    acc = accuracy_score(labels, pred_label)\n",
    "    print(\" total loss {} accuray {}\".format(loss_val, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.squeeze(class_probs[:, 1])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=probs, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
